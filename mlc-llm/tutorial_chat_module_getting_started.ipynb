{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fredf23/AIU_resume/blob/master/mlc-llm/tutorial_chat_module_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm85Ap3zDmYB"
      },
      "source": [
        "# Getting Started with MLC-LLM using the Llama 2 Model\n",
        "\n",
        "Here's a quick overview of how to get started with the MLC-LLM `ChatModule` in Python. In this tutorial, we will chat with the [Llama2](https://ai.meta.com/llama/) model. For the easiest setup, we recommend trying this out in a Google Colab notebook. Click the button below to get started!\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ttPt-hNDmYC"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Let's set up your environment, so you can successfully run the `ChatModule`. First, let's set up the Conda environment which we will be running this notebook in (not required if running in Google Colab).\n",
        "\n",
        "```bash\n",
        "conda create --name mlc-llm python=3.10\n",
        "conda activate mlc-llm\n",
        "```\n",
        "\n",
        "**Google Colab:** If you are running this in a Google Colab notebook, be sure to change your runtime to GPU by going to Runtime > Change runtime type and setting the Hardware accelerator to be \"GPU\". Select \"Connect\" on the top right to instantiate your GPU session.\n",
        "\n",
        "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the version number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KK25HZsIDmYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94edac3-b906-497f-e09c-3a5e9b7f9d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 17 04:14:13 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWOtpjJMDmYE"
      },
      "source": [
        "Next, let's download the MLC-AI and MLC-Chat nightly build packages. Go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PgW-5OAADmYE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d63da83a-ef0b-4601-eeaa-af9eb09b5a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu118-0.12.dev1689-cp310-cp310-manylinux_2_28_x86_64.whl (510.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.6/510.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlc-chat-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly_cu118-0.1.dev502-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs (from mlc-ai-nightly-cu118)\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting cloudpickle (from mlc-ai-nightly-cu118)\n",
            "  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting decorator (from mlc-ai-nightly-cu118)\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting ml-dtypes (from mlc-ai-nightly-cu118)\n",
            "  Using cached ml_dtypes-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
            "Collecting numpy (from mlc-ai-nightly-cu118)\n",
            "  Using cached numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Collecting psutil (from mlc-ai-nightly-cu118)\n",
            "  Using cached psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
            "Collecting scipy (from mlc-ai-nightly-cu118)\n",
            "  Using cached scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "Collecting tornado (from mlc-ai-nightly-cu118)\n",
            "  Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "Collecting typing-extensions (from mlc-ai-nightly-cu118)\n",
            "  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting fastapi (from mlc-chat-nightly-cu118)\n",
            "  Using cached fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
            "Collecting uvicorn (from mlc-chat-nightly-cu118)\n",
            "  Using cached uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "Collecting shortuuid (from mlc-chat-nightly-cu118)\n",
            "  Using cached shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting anyio<4.0.0,>=3.7.1 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "Collecting click>=7.0 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Collecting h11>=0.8 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Collecting idna>=2.8 (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting sniffio>=1.1 (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting exceptiongroup (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.10.1 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Using cached pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "Installing collected packages: typing-extensions, tornado, sniffio, shortuuid, psutil, numpy, idna, h11, exceptiongroup, decorator, cloudpickle, click, attrs, annotated-types, uvicorn, scipy, pydantic-core, ml-dtypes, anyio, starlette, pydantic, mlc-ai-nightly-cu118, fastapi, mlc-chat-nightly-cu118\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.8.0\n",
            "    Uninstalling typing_extensions-4.8.0:\n",
            "      Successfully uninstalled typing_extensions-4.8.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.3.3\n",
            "    Uninstalling tornado-6.3.3:\n",
            "      Successfully uninstalled tornado-6.3.3\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.0\n",
            "    Uninstalling sniffio-1.3.0:\n",
            "      Successfully uninstalled sniffio-1.3.0\n",
            "  Attempting uninstall: shortuuid\n",
            "    Found existing installation: shortuuid 1.0.11\n",
            "    Uninstalling shortuuid-1.0.11:\n",
            "      Successfully uninstalled shortuuid-1.0.11\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.6\n",
            "    Uninstalling psutil-5.9.6:\n",
            "      Successfully uninstalled psutil-5.9.6\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.1\n",
            "    Uninstalling numpy-1.26.1:\n",
            "      Successfully uninstalled numpy-1.26.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: exceptiongroup\n",
            "    Found existing installation: exceptiongroup 1.1.3\n",
            "    Uninstalling exceptiongroup-1.1.3:\n",
            "      Successfully uninstalled exceptiongroup-1.1.3\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.1.1\n",
            "    Uninstalling decorator-5.1.1:\n",
            "      Successfully uninstalled decorator-5.1.1\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.0.0\n",
            "    Uninstalling cloudpickle-3.0.0:\n",
            "      Successfully uninstalled cloudpickle-3.0.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.6.0\n",
            "    Uninstalling annotated-types-0.6.0:\n",
            "      Successfully uninstalled annotated-types-0.6.0\n",
            "  Attempting uninstall: uvicorn\n",
            "    Found existing installation: uvicorn 0.23.2\n",
            "    Uninstalling uvicorn-0.23.2:\n",
            "      Successfully uninstalled uvicorn-0.23.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.3\n",
            "    Uninstalling scipy-1.11.3:\n",
            "      Successfully uninstalled scipy-1.11.3\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.10.1\n",
            "    Uninstalling pydantic_core-2.10.1:\n",
            "      Successfully uninstalled pydantic_core-2.10.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.3.1\n",
            "    Uninstalling ml-dtypes-0.3.1:\n",
            "      Successfully uninstalled ml-dtypes-0.3.1\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.27.0\n",
            "    Uninstalling starlette-0.27.0:\n",
            "      Successfully uninstalled starlette-0.27.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.4.2\n",
            "    Uninstalling pydantic-2.4.2:\n",
            "      Successfully uninstalled pydantic-2.4.2\n",
            "  Attempting uninstall: mlc-ai-nightly-cu118\n",
            "    Found existing installation: mlc-ai-nightly-cu118 0.12.dev1689\n",
            "    Uninstalling mlc-ai-nightly-cu118-0.12.dev1689:\n",
            "      Successfully uninstalled mlc-ai-nightly-cu118-0.12.dev1689\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.103.2\n",
            "    Uninstalling fastapi-0.103.2:\n",
            "      Successfully uninstalled fastapi-0.103.2\n",
            "  Attempting uninstall: mlc-chat-nightly-cu118\n",
            "    Found existing installation: mlc-chat-nightly-cu118 0.1.dev502\n",
            "    Uninstalling mlc-chat-nightly-cu118-0.1.dev502:\n",
            "      Successfully uninstalled mlc-chat-nightly-cu118-0.1.dev502\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.1 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado==6.3.2, but you have tornado 6.3.3 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.1 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.1 which is incompatible.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.6.0 anyio-3.7.1 attrs-23.1.0 click-8.1.7 cloudpickle-3.0.0 decorator-5.1.1 exceptiongroup-1.1.3 fastapi-0.103.2 h11-0.14.0 idna-3.4 ml-dtypes-0.3.1 mlc-ai-nightly-cu118-0.12.dev1689 mlc-chat-nightly-cu118-0.1.dev502 numpy-1.26.1 psutil-5.9.6 pydantic-2.4.2 pydantic-core-2.10.1 scipy-1.11.3 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 tornado-6.3.3 typing-extensions-4.8.0 uvicorn-0.23.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator",
                  "psutil",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn7MYEFt5tvY"
      },
      "source": [
        "**Google Colab:** If in Google Colab, you may see a message warning you to restart the runtime. Simply run the following code in a new code cell to restart the runtime.\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwsWd1WbDmYE"
      },
      "source": [
        "Next, let's download the model weights for the Llama2 model and the prebuilt model libraries from Github. In order to download the large weights, we'll have to use `git lfs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppvAhErV3gjq"
      },
      "source": [
        "Note: If you are NOT running in **Google Colab** you may need to run this line `!conda install git git-lfs` to install `git` and `git-lfs` before running the following cell to fully install `git lfs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V0GjINnMDmYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8027b475-d005-4e9c-c7d6-ea09c7e5f1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYwjsCOK7Jij"
      },
      "source": [
        "These commands will download many prebuilt libraries as well as the chat configuration for Llama-2-7b that `mlc_chat` needs, which may take a long time. If in **Google Colab** you can verify that the files are being downloaded by clicking on the folder icon on the left and navigating to the `dist` and then `prebuilt` folders which should be updating as the files are being downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FSAe7Ew_DmYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f91084-5263-4133-e124-8482a52ecad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'dist/prebuilt/lib' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p dist/prebuilt\n",
        "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BDbi6H3MDmYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74615e2-9ede-41d9-9d80-2e267ab9abd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mlc-chat-Llama-2-7b-chat-hf-q4f16_1' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ru5__tDmYF"
      },
      "source": [
        "## Let's Chat!\n",
        "\n",
        "Before we can chat with the model, we must first import a library and instantiate a `ChatModule` instance. The `ChatModule` must be initialized with the appropriate model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AJAt6oW7DmYF"
      },
      "outputs": [],
      "source": [
        "from mlc_chat import ChatModule\n",
        "from mlc_chat.callback import StreamToStdout\n",
        "\n",
        "cm = ChatModule(model=\"Llama-2-7b-chat-hf-q4f16_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9m5sxyXDmYF"
      },
      "source": [
        "Note that the above invocation abstracts away the logic for finding the relevant model directory and prebuilt library paths. To specify these manually, you could run the following instead (which would be equivalent to the above).\n",
        "\n",
        "```python\n",
        "cm = ChatModule(model=\"dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\", lib_path=\"dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEaVXnnJDmYF"
      },
      "source": [
        "That is all what needed to set up the `ChatModule`. You can now chat with the model by entering any prompt you'd like. Try it out below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TNmg9N_NDmYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6305f600-a4ba-46a4-d9e1-54e418ab9a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great, thank you for asking! Python was first released in 1991 by Guido van Rossum. The first version, Python 0.9.1, was released on February 20, 1991. Since then, Python has undergone many updates and improvements, with the latest version being Python 3.10.3, which was released on August 26, 2022.\n"
          ]
        }
      ],
      "source": [
        "output = cm.generate(\n",
        "    prompt=\"When was Python released?\",\n",
        "    progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHhdhNcin7H5"
      },
      "source": [
        "You can also repeat running the code block below for multiple rounds to interact with the model in a chat style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL1gNOsFn7H6",
        "outputId": "5c8640b5-8d11-4e7a-8be9-8208caf8f1e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Cognitive comlexity\n",
            "Great, thank you for asking! Cognitive complexity refers to the degree of difficulty or challenge involved in understanding or solving a problem. It is a measure of how complex or intricate a problem is, and how much mental effort or cognitive resources are required to solve it.\n",
            "Cognitive complexity can be thought of as a multidimensional construct, with different dimensions representing different aspects of complexity. For example, there may be different dimensions for:\n",
            "1. Conceptual complexity: This refers to the number and complexity of concepts or ideas involved in a problem. For example, a problem that involves multiple variables, relationships, or concepts may have high conceptual complexity.\n",
            "2. Logical complexity: This refers to the number and complexity of logical operations or reasoning required to solve a problem. For example, a problem that involves multiple logical operations, such as AND, OR, and NOT, may have high logical complexity.\n",
            "3. Temporal complexity: This refers to the amount of time and resources required to solve a problem. For example, a problem that involves multiple steps or sub-problems may have high temporal complexity.\n",
            "4. Spatial complexity: This refers to the amount of space or geography involved in a problem. For example, a problem that involves multiple locations or geographical areas may have high spatial complexity.\n",
            "5. Syntactic complexity: This refers to the number and complexity of linguistic structures involved in a problem. For example, a problem that involves complex sentence structures or ambiguous language may have high syntactic complexity.\n",
            "6. Semantic complexity: This refers to the number and complexity of meanings or interpretations involved in a problem. For example, a problem that involves multiple meanings or interpretations of a term may have high semantic complexity.\n",
            "7. Pragmatic complexity: This refers to the number and complexity of social or cultural norms involved in a problem. For example, a problem that involves multiple social or cultural norms may have high pragmatic complexity.\n",
            "8. Cognitive load: This refers to the amount of mental effort or cognitive load required to solve a problem. For example, a problem that involves multiple tasks or sub-problems may have high cognitive load.\n",
            "9. Knowledge complexity: This refers to the number and complexity of knowledge domains involved in a problem. For example, a problem that involves multiple knowledge domains, such as physics, biology, and mathematics, may have high knowledge complexity.\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "# @title Текст заголовка по умолчанию\n",
        "prompt = input(\"Prompt: \")o\n",
        "output = cm.generate(prompt=prompt, progress_callback=StreamToStdout(callback_interval=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJdy8juEn7H6",
        "outputId": "ad4817d8-bcae-4a7c-9adf-d492912ba98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a summary of my response:\n",
            "Cognitive complexity refers to the difficulty or challenge involved in understanding or solving a problem, which can be measured across different dimensions such as conceptual, logical, temporal, spatial, syntactic, semantic, pragmatic, cognitive load, and knowledge complexity. Understanding and managing cognitive complexity is crucial for problem-solving, learning, and decision-making in various domains. By recognizing and categorizing cognitive complexity, individuals and organizations can better navigate complex problems and make informed decisions.\n"
          ]
        }
      ],
      "source": [
        "output = cm.generate(\n",
        "    prompt=\"Please summarize your response in three sentences.\",\n",
        "    progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4bOyUk7DmYF"
      },
      "source": [
        "To check the generation speed of the chat bot, you can print the statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PPbPj6vpDmYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f5c6b8-475a-4e40-a992-264872d0202e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prefill: 103.5 tok/s, decode: 27.0 tok/s\n"
          ]
        }
      ],
      "source": [
        "print(cm.stats())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAb-XZPnDmYF"
      },
      "source": [
        "By default, the `ChatModule` will keep a history of your chat. You can reset the chat history by running the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKpKgVxNDmYF"
      },
      "outputs": [],
      "source": [
        "cm.reset_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3NzQulYn7H-"
      },
      "source": [
        "### Benchmark Performance\n",
        "\n",
        "To benchmark the performance, we can use the `benchmark_generate` method of ChatModule. It takes an input prompt and the number of tokens to generate, ignores the system prompt and model stop criterion, generates tokens in a language model way and stops until finishing generating the desired number of tokens. After calling `benchmark_generate`, we can use `stats` to check the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "24FjfhMQn7H-",
        "outputId": "d1ad66ac-d86a-440d-d69a-74e421f3675b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " everybody uses benchmark to compare the performance of different systems or algorithms, but what is it really?\n",
            "\n",
            "Benchmarking is the process of measuring the performance of a system, application, or algorithm by running it through a series of predefined tests or tasks. The results of these tests are then analyzed to determine the performance of the system, application, or algorithm in various areas, such as speed, efficiency, scalability, and reliability.\n",
            "The goal of benchmarking is to provide a fair and consistent comparison of different systems or algorithms, allowing users to make informed decisions about which one is best suited for their specific needs. Benchmarking can be used in a variety of fields, including computer science, engineering, finance, and medicine, and is an essential tool for evaluating the performance of complex systems and algorithms.\n",
            "There are several types of benchmarking, including:\n",
            "1. Absolute benchmarking: This involves measuring the performance of a system or algorithm by running it through a series of predefined tests or tasks, and comparing the results to those of other systems or algorithms.\n",
            "2. Relative benchmarking: This involves measuring the performance of a system or algorithm relative to other systems or algorithms in the same class or category.\n",
            "3. Competitive benchmarking: This involves measuring the performance of a system or algorithm in competition with other systems or algorithms, often in a real-world setting.\n",
            "4. Functional benchmarking: This involves measuring the performance of a system or algorithm in terms of its ability to perform specific functions or tasks.\n",
            "5. Non-functional benchmarking: This involves measuring the performance of a system or algorithm in terms of its non-functional properties, such as scalability, reliability, and security.\n",
            "Benchmarking can be performed using a variety of methods, including:\n",
            "1. Synthetic benchmarking: This involves creating artificial test cases or scenarios to measure the performance of a system or algorithm.\n",
            "2. Real-world benchmarking: This involves measuring the performance of a system or algorithm in a real-world setting, often by collecting data from actual users or customers.\n",
            "3. Hybrid benchmarking: This involves combining synthetic and real-world benchmarking methods to obtain a more comprehensive view of a system or algorithm's performance.\n",
            "Benchmarking can be used for a variety of purposes, including:\n",
            "1. Evaluating the performance of a new system or algorithm:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'prefill: 40.6 tok/s, decode: 30.3 tok/s'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "print(cm.benchmark_generate(prompt=\"What is benchmark?\", generate_length=512))\n",
        "cm.stats()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}